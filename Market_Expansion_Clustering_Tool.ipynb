{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Market Expansion Clustering Tool.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPBQVYpLbkRC",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "## Description\n",
        "A general clustering tool to support non-technical Operations Managers in making informed new market expansion decisions by clustering demand with a built in cost model for new market comparison\n",
        "\n",
        "## Purpose\n",
        "Allow dynamic market construction through flexible geographical size while identifying optimally dense clusters or \"markets\" as expansion candidates\n",
        "\n",
        "## Product\n",
        "Output is a series of spreadsheets that can be fed into a Kepler interactive map to facilitate a more intuitive and actionable understanding of latent out of territory demand.\n",
        "\n",
        "[Example map link](https://drive.google.com/open?id=19Y6ITL2OBuzE_A6xF6qliddHPztV-mjY)\n",
        "\n",
        "# How to Use\n",
        "**Watch the tutorial videos: [Part 1](https://www.loom.com/share/25b52af60ff24859bdca26f12c7f5e86) and [Part 2](https://www.loom.com/share/8e76a084e2314faf94995f70be2795a2)**\n",
        "\n",
        "## What you will need\n",
        "1. A leads.csv file containing the sales lead data you want to cluster, including at a minimum the following columns: lead_id, zipcode, and status. DO NOT include columns with the names city, state, lat, or lonâ€“these are added automatically. You can optionally include additional columns to include in the map tool tip.\n",
        "2. An offices.csv file containing the relative efficiencies of each of your current offices/warehouses along with their names and zipcodes\n",
        "3. A territory.csv file containing all the zipcodes you currently service as in-territory and the office name associated with each one.\n",
        "\n",
        "## Instructions\n",
        "1. Make a copy of this iPython notebook and save it in a folder in your google drive\n",
        "2. Upload the three csv data files to the same folder you created for the copy of this tool\n",
        "3. Change the data directory in the first cell below to the right folder path you created.\n",
        "4. Open the \"Runtime\" menu in this script and click \"Run all\"\n",
        "5. Scroll down and watch for cell output or errors. If none, then check your folder for the output files!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzdZ8e9HrKaW",
        "colab_type": "text"
      },
      "source": [
        "### Environment Setup\n",
        "Here you link this tool with the folder or file directory containing your datasets and install necessary dependencies and libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fHouMMCUxpX",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "#Change to your data directory\n",
        "os.chdir('/content/drive/My Drive/Example')\n",
        "#List all files. You should see your lead data in here\n",
        "!ls '/content/drive/My Drive/Example'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n-vfYnhbkRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install geopy\n",
        "!pip install shapely\n",
        "!pip install uszipcode\n",
        "!pip install hdbscan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXFaT_l16RPY",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from uszipcode import SearchEngine\n",
        "from math import cos, asin, sqrt\n",
        "from geopy.distance import great_circle\n",
        "from shapely.geometry import MultiPoint\n",
        "import hdbscan\n",
        "import scipy.cluster as cluster\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import math\n",
        "from geopy.geocoders import Nominatim\n",
        "import re\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oATrwLUrwao",
        "colab_type": "text"
      },
      "source": [
        "### Zipcode Geocoding\n",
        "These functions are used for finding the latitude and longitude for each zipcode in your provided datasets, which we will import in the next section. The clean_zip function simply makes sure all the zipcodes have the same format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbtoEQq56RPj",
        "colab": {}
      },
      "source": [
        "#Primary zipcode lookup function: using USZipcode library\n",
        "def usz_lookup(z):\n",
        "    search = SearchEngine(simple_zipcode=True)\n",
        "    zip_dict = search.by_zipcode(z).to_dict()\n",
        "    return zip_dict['major_city'], zip_dict['state'], zip_dict['lat'], zip_dict['lng']\n",
        "\n",
        "#Secondary lookup function: using Geopy Nominatim\n",
        "def nom_lookup(z):\n",
        "    geolocator = Nominatim(user_agent=\"_\", country_bias='US')\n",
        "    try:\n",
        "        location = geolocator.geocode(z, exactly_one=True)\n",
        "        return location.latitude, location.longitude\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def geocode_zip(data, drop=True):\n",
        "    failed_zips = []\n",
        "    #initialize columns for lookup\n",
        "    if any(col in data.columns for col in ['city','state','lat','lon']):\n",
        "      raise NameError('Remove city, state, lat, and lon columns from your leads.csv')\n",
        "    data = data.reindex(columns = data.columns.tolist() + ['city','state','lat','lon'])\n",
        "    for z in data['zipcode'].unique():\n",
        "        #Try primary lookup\n",
        "        data.loc[data['zipcode']==z,['city','state','lat','lon']] = usz_lookup(z)\n",
        "        #if no latitude returned, try secondary lookup \n",
        "        if pd.isna(data.loc[data['zipcode']==z,'lat']).any():\n",
        "            data.loc[data['zipcode']==z,['lat','lon']] = nom_lookup(z)\n",
        "            if pd.isna(data.loc[data['zipcode']==z,'lat']).any():\n",
        "                failed_zips.append(z)\n",
        "                data = data.drop(data.index[data['zipcode']==z])\n",
        "    if len(failed_zips) > 0:\n",
        "      print('zipcodes dropped (no coordinates): ', failed_zips)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RP1cBw8bkRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_zip(data):\n",
        "    #if all zips are 5 digits, skip this step\n",
        "    data = data.astype({'zipcode':str})\n",
        "    if len(data.loc[data['zipcode'].str.len()>5,'zipcode'])==0:\n",
        "      return data\n",
        "    #checks for zipcodes with suffixes, replaces them with first 5 digits\n",
        "    data.loc[data['zipcode'].str.len()>5,'zipcode'] = data['zipcode'].astype(str).apply(lambda x: x[:x.find('-')])\n",
        "    #adds a leading zero for zipcodes with only 4 digits\n",
        "    data['zipcode'] = data['zipcode'].str.zfill(5)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1XgkHzPbkRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input leads data: name file 'leads.csv'. If csv not generated from Microsoft Excel, pass excel = False\n",
        "#Needs to contain columns: lead_id (unique), zipcode, status\n",
        "def read_leads(territory, excel=True):\n",
        "    if excel:\n",
        "        data = pd.read_csv('leads.csv', encoding = \"ISO-8859-1\")\n",
        "    else:\n",
        "        data = pd.read_csv('leads.csv')\n",
        "    data = clean_zip(data)\n",
        "    #checking for zipcodes to drop\n",
        "    rows = len(data)\n",
        "    data = data.drop_duplicates(subset='lead_id')\n",
        "    print('Duplicates dropped: ', rows - len(data))\n",
        "    rows = len(data)\n",
        "    data = data.dropna(subset=['zipcode'], how='any')\n",
        "    print('No-zipcode rows dropped: ', rows - len(data))\n",
        "    data['serviceable'] = [1 if zp in (territory['zipcode']) else 0 for zp in data['zipcode']]\n",
        "    data = geocode_zip(data)\n",
        "    #get job (lead_id) counts per zipcode\n",
        "    job_counts = data.groupby(['zipcode','status'])['lead_id'].count().reset_index()\n",
        "    #maps the number of active and cancelled jobs in each zipcode to leads dataframe\n",
        "    data['zip_active_jobs'] = [job_counts.loc[(job_counts['zipcode']==zp) & (job_counts['status']=='active'),'lead_id'].values[0]\\\n",
        "                               if zp in data.loc[data['status']=='active','zipcode'].values else 0 for zp in data['zipcode'].values]\n",
        "    data['zip_cancelled_jobs'] = [job_counts.loc[(job_counts['zipcode']==zp) & (job_counts['status']=='cancelled'),'lead_id'].values[0]\\\n",
        "                               if zp in data.loc[data['status']=='cancelled','zipcode'].values else 0 for zp in data['zipcode'].values]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83elia4qlZB",
        "colab_type": "text"
      },
      "source": [
        "## Data Import\n",
        "Here is where your datasets are read by the tool and cleaned using the above functions. If the below cell fails to run, check your file names and the column names in each file to make sure they contain the necessary datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUFcgdICbkRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input territory data: name file 'territory.csv'\n",
        "territory = clean_zip(pd.read_csv('territory.csv'))\n",
        "\n",
        "#Input leads data: name file 'leads.csv'\n",
        "leads = read_leads(territory)\n",
        "\n",
        "#Input office information, reformatting, calculating lat/lon.\n",
        "offices = geocode_zip(clean_zip(pd.read_csv('offices.csv')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evSGkvXvqkpv",
        "colab_type": "text"
      },
      "source": [
        "### Clustering\n",
        "The **agglomerative** function performs hierarchical complete-linkage clustering on the leads you provided, grouping them into markets based on the size that you specify. The get_cluster_points function adds the cluster label to each lead based on which cluster/market it was grouped into.\n",
        "\n",
        "Optionally, you can use **HDBSCAN** to cluster your leads instead, which is recommended if the maximum size and shape of the clusters are not a constraint for your business (warehouse hub-and-spoke model not necessary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2yuiF2RbkRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cluster_leads(leads, size, viz=False):\n",
        "    #complete linkage clustering. Filters out currently serviceable leads (in territory)\n",
        "    coordinates = np.array([tuple(row) for row in leads.loc[leads['serviceable']==0,['lat','lon']].values])\n",
        "    #print(len(coordinates[np.isnan(coordinates)]))\n",
        "    link = linkage(np.radians(coordinates),method='complete')\n",
        "    #outputs visualization of dendrogram/tree\n",
        "    if viz:\n",
        "      print('Drawing dendrogram')\n",
        "      dend = dendrogram(link)\n",
        "    #cut the tree, return cluster labels\n",
        "    cluster_labels = cluster.hierarchy.cut_tree(link, height=size)\n",
        "    #return number of unique clusters\n",
        "    cluster_count = len(np.unique(cluster_labels))\n",
        "    cluster_labels = np.concatenate(cluster_labels)\n",
        "    #return series of lead coordinates in each cluster\n",
        "    clusters = pd.Series([coordinates[cluster_labels==n] for n in range(cluster_count)])\n",
        "    return clusters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVO84QCObkRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#returns dataframe with all the leads in a cluster\n",
        "def get_cluster_points(clusters):\n",
        "    cluster_points = pd.DataFrame({'cluster': [x for x in range(len(clusters))],'lat' : [[x[0] for x in row] for row in clusters],\\\n",
        "                              'lon' : [[x[1] for x in row] for row in clusters]})\n",
        "    #flattening lists\n",
        "    cluster_points = pd.DataFrame({\n",
        "        'cluster':np.repeat(cluster_points['cluster'].values, cluster_points['lat'].str.len())}\n",
        "        ).assign(**{'lat':np.concatenate(cluster_points['lat'].values),\n",
        "                    'lon':np.concatenate(cluster_points['lon'].values)})#[cluster_points.columns]\n",
        "    #add cluster to leads df\n",
        "    leads['cluster'] = leads.apply(lambda row: cluster_points.loc[(cluster_points['lat']==row['lat']) & (cluster_points['lon']==row['lon']),'cluster'].iloc[0], axis=1)\n",
        "    return leads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-mdhXyi6RP2",
        "colab": {}
      },
      "source": [
        "def agglomerative(leads):\n",
        "  #Cut tree for a certain height (h), where h == distance between farthest points in a cluster. Determines cluster size\n",
        "  miles = int(input('Enter cluster diameter in miles: '))\n",
        "  #converts miles to radians\n",
        "  h = miles/3959.\n",
        "  #warn user of potentially long loading time\n",
        "  print('Clustering leads now. This may take some time.')\n",
        "  #series of leads in each cluster\n",
        "  clusters = cluster_leads(leads, h)\n",
        "  #add cluster information to leads df\n",
        "  leads = get_cluster_points(clusters)\n",
        "  return clusters, leads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC34hr1zrU3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hdensity_based(leads):\n",
        "  size = int(input('Enter minimum cluster size (number of leads): '))\n",
        "  clusterer = hdbscan.HDBSCAN(min_cluster_size=size)\n",
        "  #execute clustering\n",
        "  print('Clustering leads now. This may take some time.')\n",
        "  cluster_labels = clusterer.fit_predict(leads[['lat','lon']])\n",
        "  #cluster count excludes unclustered \"noisy\" leads\n",
        "  cluster_count = len(np.unique(cluster_labels))-1\n",
        "  #series of leads in each cluster\n",
        "  coordinates = np.array([tuple(row) for row in leads.loc[leads['serviceable']==0,['lat','lon']].values])\n",
        "  clusters = pd.Series([coordinates[cluster_labels==n] for n in range(cluster_count)])\n",
        "  #add cluster information to leads df\n",
        "  leads['cluster'] = cluster_labels\n",
        "  return clusters, leads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU98H5D6ypqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def execute_clustering(leads):\n",
        "  method = str(input('Enter \"a\" for agglomerative or \"d\" for HDBSCAN: '))\n",
        "  if method == 'a':\n",
        "    clusters, leads = agglomerative(leads)\n",
        "    return clusters, leads\n",
        "  elif method == 'd':\n",
        "    clusters, leads = hdensity_based(leads)\n",
        "    return clusters, leads\n",
        "  else:\n",
        "    raise(NameError('Invalid method selection: re-run this cell and enter either \"a\" or \"d\"'))\n",
        "  \n",
        "clusters, test_leads = execute_clustering(leads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fque27vGuJUk",
        "colab_type": "text"
      },
      "source": [
        "### Mapping to Closest Office\n",
        "\n",
        "These functions perform a distance calculation from each market's centerpoint to your nearest office, which will be an important input to the cost model and eventual cluster map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_8WBSOUl6RQL",
        "colab": {}
      },
      "source": [
        "#functions tofind closest office distance and coordinates for each cluster\n",
        "#geodetic distance calculation\n",
        "def distance(lat1, lon1, lat2, lon2):\n",
        "    p = 0.017453292519943295\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n",
        "    return 2 * 3959 * asin(sqrt(a))\n",
        "\n",
        "#returns the minimum distance, closest office coordinates, and efficiency\n",
        "def closest(offices, center):\n",
        "    dist = np.inf\n",
        "    for i in range(len(offices)):\n",
        "        dist2 = distance(center['lat'],center['lon'],offices.loc[i,'lat'],offices.loc[i,'lon'])\n",
        "        if dist2 < dist:\n",
        "            dist = dist2\n",
        "            coords = [offices.loc[i,'lat'], offices.loc[i,'lon']]\n",
        "            name = offices.loc[i,'office_name']\n",
        "            eff = offices.loc[i,'efficiency']\n",
        "    return dist, coords, name, eff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2x6Krdnufw_",
        "colab_type": "text"
      },
      "source": [
        "### Creating Map Data Structure\n",
        "The get_centermost_point function finds the representative or center point of each market to visualize on the map. The make_cluster_map function calculates active and cancelled job counts and creates a data table with each of the cluster centerpoints and their associated metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "etSI4kM36RQF",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#build dataframe from cluster output for mapping.\n",
        "def get_centermost_point(cluster):\n",
        "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
        "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
        "    return centermost_point[0], centermost_point[1]\n",
        "\n",
        "def make_cluster_map(clusters, offices, cluster_points):\n",
        "    centroids = clusters.map(get_centermost_point)\n",
        "    lats, lons = zip(*centroids)\n",
        "    rep_points = pd.DataFrame({'lon':lons, 'lat':lats, 'closest_distance':'', 'closest_lat':'', \\\n",
        "                               'closest_lon':'', 'office_name': '', 'office_eff':'', 'active_jobs':'',\\\n",
        "                               'cancelled_jobs':''})\n",
        "    #Calculating active and cancelled job counts    \n",
        "    job_counts = cluster_points.groupby(['cluster','zipcode'])['zip_active_jobs','zip_cancelled_jobs']\\\n",
        "                    .mean().reset_index()\n",
        "    for i in range(len(rep_points)):\n",
        "        center = {'lat' : rep_points.loc[i,'lat'], 'lon': rep_points.loc[i,'lon']}\n",
        "        x = closest(offices, center)\n",
        "        rep_points.loc[i,['closest_distance','closest_lat','closest_lon', 'office_name', 'office_eff','active_jobs',\\\n",
        "                          'cancelled_jobs']] = round(x[0],2), x[1][0], x[1][1], x[2], x[3], \\\n",
        "                            job_counts.loc[job_counts['cluster']==i,'zip_active_jobs'].sum(), \\\n",
        "                            job_counts.loc[job_counts['cluster']==i,'zip_cancelled_jobs'].sum()\n",
        "    return rep_points        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BcCKBT5bkRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_map = make_cluster_map(clusters, offices, leads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqXBtvFXvs1I",
        "colab_type": "text"
      },
      "source": [
        "### Cost Modeling\n",
        "\n",
        "This section creates the embedded cost model for evaluating each cluster on a per-job cost level. The descriptions for each input are commented in the code below. Make any relevant changes in the section indicated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DalzrpLO6RQN",
        "colab": {}
      },
      "source": [
        "#### MODIFY COSTS AND ASSUMPTIONS HERE ####\n",
        "\n",
        "#Cost model\n",
        "crew_size = 5 #members per crew\n",
        "hours = 10 #hours/crew member/day\n",
        "\n",
        "#Cost Inputs\n",
        "burden = 1.2 #burden added to operations costs\n",
        "inhouse_rate = 30 #cost/hour/crew member, taken from efficiency metrics\n",
        "hotel = 100 #cost/day/crew member, taken from Seattle traveling crew hotel costs\n",
        "per_diem = 30 #cost/day/crew member\n",
        "consumables = 200 #cost/job\n",
        "\n",
        "#Assumptions\n",
        "flight_thrsh = 500 #distance in miles away from office beyond which crews fly\n",
        "flight_time = 0.5 #hours per hundred miles\n",
        "airport_time = 3 #hours spent traveling not on the flight\n",
        "gas = 0.2 #dollars per mile if driving\n",
        "driving_speed = 60 #in mph\n",
        "\n",
        "####\n",
        "\n",
        "def cost_model(data):\n",
        "    data = data.reindex(columns = data.columns.to_list()+['days','labor','hotel','travel','perdiem',\\\n",
        "                                           'consumables','cluster_cost','job_cost'])\n",
        "    for i in range(len(data)):\n",
        "        jobs = data.loc[i,'active_jobs']\n",
        "        eff = data.loc[i,'office_eff']\n",
        "        days = (jobs*eff)/(crew_size*hours)\n",
        "        labor_cost = inhouse_rate*crew_size*hours*days*burden\n",
        "        hotel_cost = hotel*crew_size*math.ceil(days)\n",
        "        distance = data.loc[i,'closest_distance']\n",
        "        if  distance > flight_thrsh:\n",
        "            #fly\n",
        "            flights= crew_size*(50. + (0.11*distance))\n",
        "            fly_hours = crew_size*inhouse_rate*burden*((flight_time*distance)+airport_time)\n",
        "            travel_cost = flights + fly_hours\n",
        "        else:\n",
        "            #drive\n",
        "            gas_cost = gas*distance\n",
        "            drive_hours = crew_size*inhouse_rate*burden*(distance/driving_speed)\n",
        "            travel_cost = gas_cost+drive_hours\n",
        "        per_diem_cost = per_diem*crew_size*days\n",
        "        consumables_cost = consumables*jobs\n",
        "        if jobs > 0:\n",
        "            total_cost = sum([labor_cost,hotel_cost,travel_cost,per_diem_cost,consumables_cost])\n",
        "            job_cost = total_cost/jobs\n",
        "        else:\n",
        "            total_cost = 0.\n",
        "            job_cost = 0.\n",
        "        #adding costs to data frame\n",
        "        data.loc[i,['days','labor','hotel','travel','perdiem','consumables','cluster_cost',\\\n",
        "                    'job_cost']] = (days, labor_cost, hotel_cost, travel_cost, per_diem_cost, consumables_cost, round(total_cost,2), round(job_cost,2))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "faXjep3U6RQT",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "cost_cluster_map = cost_model(cluster_map)#, cost_inputs())\n",
        "print(cost_cluster_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDu_NJaGtue",
        "colab_type": "text"
      },
      "source": [
        "### Export Model Output\n",
        "These lines create a csv file for each of the complete clustered leads data table, the offices data table, and the costed cluster mapping table. You can find the files in your google drive working directory that you originally linked colab to in the \"Environment Setup\" section above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BUvnJdA96RQw",
        "colab": {}
      },
      "source": [
        "#exporting data tables as csvs for reporting and Kepler\n",
        "leads.to_csv('cluster_points.csv')\n",
        "offices.to_csv('geocoded_offices.csv')\n",
        "cost_cluster_map.to_csv('clusters.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkN7k8fUsKgY",
        "colab_type": "text"
      },
      "source": [
        "### Data Generation\n",
        "These function generate the fake datasets used in the training videos. They are included here for reference only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Skpd2k63WNbE",
        "colab": {}
      },
      "source": [
        "#Generating Example Data\n",
        "#Territory data\n",
        "def territory_gen():\n",
        "    #lookup function for zipcodes\n",
        "    search = SearchEngine()\n",
        "    #Generating territory for a solar contractor based in Los Angeles, California.\n",
        "    #Services top 25 most populated zipcodes in LA and SF area\n",
        "    LA_res = search.by_city_and_state('Los Angeles', 'CA', sort_by=\"population\", ascending=False, returns=25)\n",
        "    territory = pd.DataFrame({'zipcode':[x.zipcode for x in LA_res], 'city':'Los Angeles', 'state':'CA', \\\n",
        "                              'office_name':'LA_branch','office_street':'111 Stuart Ave',\\\n",
        "                              'office_city':'Los Angeles','office_state':'CA','serviceable':1})\n",
        "    SF_res = search.by_city_and_state('San Francisco', 'CA', sort_by=\"population\", ascending=False, returns=25)\n",
        "    territory = territory.append(pd.DataFrame({'zipcode':[x.zipcode for x in SF_res], 'city':'San Francisco', 'state':'CA', \\\n",
        "                              'office_name':'SF_branch','office_street':'12 Monte Carlo',\\\n",
        "                              'office_city':'San Francisco','office_state':'CA','serviceable':1}), ignore_index=True)\n",
        "    return territory\n",
        "    \n",
        "#Leads for clustering.\n",
        "def zip_gen(state):\n",
        "    search = SearchEngine()\n",
        "    #unique list of zip codes in a state\n",
        "    zip_list = set([x.zipcode for x in search.by_state(state, returns=0)])\n",
        "    #holding list for zip codes of generated jobs\n",
        "    job_zips = []\n",
        "    #cap of 1000 leads in each state\n",
        "    while len(job_zips)<1000:\n",
        "        zp = random.sample(zip_list,1)[0]\n",
        "        #up to 30 jobs in each zipcode\n",
        "        for i in range(random.randint(1,31)):\n",
        "            job_zips.append(zp)\n",
        "    return job_zips\n",
        "\n",
        "def lead_gen(territory):\n",
        "    #generate zipcodes with artificial demand\n",
        "    zips = zip_gen('NV') + zip_gen('AZ') + zip_gen('CA') + zip_gen('WA') + zip_gen('OR')\n",
        "    #generating a lead dataframe of similar structure as the one uploaded\n",
        "    leads = pd.DataFrame({'lead_id':random.sample(list(range(10000)),len(zips)),'zipcode':zips,\\\n",
        "                  'status':[random.sample(['active','cancelled'],1)[0] for x in range(len(zips))]})\n",
        "    # leads['serviceable'] = [1 if zp in territory.loc[territory['serviceable']==1,'zipcode'] \\\n",
        "    #                         else 0 for zp in leads['zipcode'].values]\n",
        "    # #get job (lead_id) counts per zipcode\n",
        "    # job_counts = leads.groupby(['zipcode','status'])['lead_id'].count().reset_index()\n",
        "    # leads['zip_active_jobs'] = [job_counts.loc[(job_counts['zipcode']==zp) & (job_counts['status']=='active'),'lead_id'] for zp in leads['zipcode']]\n",
        "    # leads['zip_cancelled_jobs'] = [job_counts.loc[(job_counts['zipcode']==zp) & (job_counts['status']=='cancelled'),'lead_id'] for zp in leads['zipcode']]\n",
        "    return leads\n",
        "\n",
        "def office_gen():\n",
        "    #minimum columns include office name, zipcode, and efficiency (work hours/job)\n",
        "    offices = pd.DataFrame({'office_name':['San Francisco','Los Angeles'], 'zipcode':['94102','90089'], 'efficiency':[30,20]})\n",
        "    return offices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01BbLpPfsmZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "territory = territory_gen()\n",
        "leads = lead_gen(territory)\n",
        "offices = office_gen()\n",
        "\n",
        "#Exporting to csvs\n",
        "territory.to_csv('territory.csv')\n",
        "leads.to_csv('leads.csv')\n",
        "offices.to_csv('offices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}